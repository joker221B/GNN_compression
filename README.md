# GNN_compression

Knowledge distillation approaches have mainly ignored graph neural networks (GNN), which deal with non-grid data. Knowledge distillation concentrate on convolu- tional neural networks (CNNs) where the input samples are in a grid domain, such as photos. We have im- plemented the dedicated strategy of distilling knowledge from a pre-trained GNN model. The proposed method is a local structure preserving module that explicitly accounts for the topological semantics of the teacher GNN in order to facilitate knowledge transfer from the teacher GNN to the student. In this module, both the teacher and the student’s local structure information is extracted as distributions, and by computing similar- ity scores between these distributions, topology-aware knowledge transfer from the teacher is enabled, result- ing in a compact yet high-performance student model. We evaluated our model on Protein-Protein Interaction Dataset (PPI), which is commonly used for node classifi- cation tasks. We achieved 20x compression with our stu- dent model with a minor drop of 2.24% accuracy (from 98.07% to 95.83%)

We implemented a compression technique for graph neu- ral network using knowledge distillation. The aggrega- tion approach in GNN is critical for incorporating the node characteristics that are learned during the training process. However, distilling information that accurately describes the aggregation function and transferring it to the student is difficult. This is accomplished by preserv- ing the teacher network local structure during the training process. The local structure of the intermediate feature maps is represented as distributions over the similaritiesbetween the local structure’s center node and its neigh- bors, therefore preserving the local structures is equal to matching the distributions. Experiments on the PPI dataset demonstrate that the method yields compression of 20x with a minor drop of 2% accuracy.

